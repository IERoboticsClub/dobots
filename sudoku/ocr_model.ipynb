{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Model for OCR\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to train the model used for the ocr (ocr_model.h5 parameters file).\n",
    "\n",
    "Extra dependencies needed: scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Prepare the digits dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Datapoints =  10160\n"
     ]
    }
   ],
   "source": [
    "# Source dataset: https://www.kaggle.com/datasets/karnikakapoor/digits\n",
    "# Original dataset is from Charst74K: http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/\n",
    "\n",
    "dataset_path = \"./assets/Digits/\"\n",
    "\n",
    "data = os.listdir(dataset_path)\n",
    "data_X = []     \n",
    "data_y = []  \n",
    "data_classes = len(data)\n",
    "for i in range (0,data_classes):\n",
    "    data_list = os.listdir(dataset_path +\"/\"+str(i))\n",
    "    for j in data_list:\n",
    "        pic = cv2.imread(dataset_path +\"/\"+str(i)+\"/\"+j)\n",
    "        pic = cv2.resize(pic,(64,64))\n",
    "        data_X.append(pic)\n",
    "        data_y.append(i)\n",
    "\n",
    "# Labels and images\n",
    "data_X = np.array(data_X)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "if len(data_X) == len(data_y) :\n",
    "    print(\"Total Datapoints = \",len(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape =  (8636, 64, 64, 3)\n",
      "Test Set Shape =  (1524, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data_X,data_y,test_size=0.15)\n",
    "print(\"Training Set Shape = \",train_X.shape)\n",
    "print(\"Test Set Shape = \",test_X.shape)\n",
    "\n",
    "# Preprocessing the images for neuralnet\n",
    "\n",
    "def Prep(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #making image grayscale\n",
    "    img = cv2.equalizeHist(img) #Histogram equalization to enhance contrast\n",
    "    img = img/255 #normalizing\n",
    "    return img\n",
    "\n",
    "train_X = np.array(list(map(Prep, train_X)))\n",
    "test_X = np.array(list(map(Prep, test_X)))\n",
    "\n",
    "#Reshaping the images\n",
    "train_X = train_X.reshape(train_X.shape[0], train_X.shape[1], train_X.shape[2],1)\n",
    "test_X = test_X.reshape(test_X.shape[0], test_X.shape[1], test_X.shape[2],1)\n",
    "\n",
    "#Augmentation\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, shear_range=0.1, rotation_range=10)\n",
    "datagen.fit(train_X)\n",
    "\n",
    "# One hot encoding of the labels\n",
    "train_y = to_categorical(train_y, data_classes)\n",
    "test_y = to_categorical(test_y, data_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_56 (Conv2D)          (None, 64, 64, 60)        1560      \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 64, 64, 60)        90060     \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 32, 32, 60)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 32, 32, 30)        16230     \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 32, 32, 30)        8130      \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 16, 16, 30)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 16, 16, 30)        0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 7680)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 100)               768100    \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 885,090\n",
      "Trainable params: 885,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture is based on LeNet-5\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add((Conv2D(60,(5,5),input_shape=(64, 64, 1) ,padding = 'same' ,activation='relu')))\n",
    "model.add((Conv2D(60, (5,5),padding=\"same\",activation='relu')))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add((Conv2D(30, (3,3), padding=\"same\", activation='relu')))\n",
    "model.add((Conv2D(30, (3,3), padding=\"same\", activation='relu')))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 06:20:52.179655: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 - 138s - loss: 0.3916 - accuracy: 0.8816 - 138s/epoch - 691ms/step\n",
      "Epoch 2/30\n",
      "200/200 - 941s - loss: 0.2460 - accuracy: 0.9287 - 941s/epoch - 5s/step\n",
      "Epoch 3/30\n",
      "200/200 - 109s - loss: 0.2274 - accuracy: 0.9333 - 109s/epoch - 546ms/step\n",
      "Epoch 4/30\n",
      "200/200 - 94s - loss: 0.1834 - accuracy: 0.9459 - 94s/epoch - 469ms/step\n",
      "Epoch 5/30\n",
      "200/200 - 93s - loss: 0.1587 - accuracy: 0.9511 - 93s/epoch - 467ms/step\n",
      "Epoch 6/30\n",
      "200/200 - 94s - loss: 0.1488 - accuracy: 0.9561 - 94s/epoch - 470ms/step\n",
      "Epoch 7/30\n",
      "200/200 - 94s - loss: 0.1356 - accuracy: 0.9614 - 94s/epoch - 470ms/step\n",
      "Epoch 8/30\n",
      "200/200 - 95s - loss: 0.1292 - accuracy: 0.9619 - 95s/epoch - 474ms/step\n",
      "Epoch 9/30\n",
      "200/200 - 97s - loss: 0.1189 - accuracy: 0.9656 - 97s/epoch - 486ms/step\n",
      "Epoch 10/30\n",
      "200/200 - 1906s - loss: 0.1153 - accuracy: 0.9681 - 1906s/epoch - 10s/step\n",
      "Epoch 11/30\n",
      "200/200 - 332s - loss: 0.1128 - accuracy: 0.9673 - 332s/epoch - 2s/step\n",
      "Epoch 12/30\n",
      "200/200 - 2108s - loss: 0.1158 - accuracy: 0.9675 - 2108s/epoch - 11s/step\n",
      "Epoch 13/30\n",
      "200/200 - 2102s - loss: 0.1072 - accuracy: 0.9679 - 2102s/epoch - 11s/step\n",
      "Epoch 14/30\n",
      "200/200 - 328s - loss: 0.1184 - accuracy: 0.9719 - 328s/epoch - 2s/step\n",
      "Epoch 15/30\n",
      "200/200 - 317s - loss: 0.1045 - accuracy: 0.9690 - 317s/epoch - 2s/step\n",
      "Epoch 16/30\n",
      "200/200 - 316s - loss: 0.1064 - accuracy: 0.9703 - 316s/epoch - 2s/step\n",
      "Epoch 17/30\n",
      "200/200 - 2878s - loss: 0.0987 - accuracy: 0.9720 - 2878s/epoch - 14s/step\n",
      "Epoch 18/30\n",
      "200/200 - 324s - loss: 0.1044 - accuracy: 0.9730 - 324s/epoch - 2s/step\n",
      "Epoch 19/30\n",
      "200/200 - 289s - loss: 0.1077 - accuracy: 0.9705 - 289s/epoch - 1s/step\n",
      "Epoch 20/30\n",
      "200/200 - 315s - loss: 0.0835 - accuracy: 0.9769 - 315s/epoch - 2s/step\n",
      "Epoch 21/30\n",
      "200/200 - 2044s - loss: 0.1179 - accuracy: 0.9681 - 2044s/epoch - 10s/step\n",
      "Epoch 22/30\n",
      "200/200 - 332s - loss: 0.1070 - accuracy: 0.9717 - 332s/epoch - 2s/step\n",
      "Epoch 23/30\n",
      "200/200 - 328s - loss: 0.0899 - accuracy: 0.9736 - 328s/epoch - 2s/step\n",
      "Epoch 24/30\n",
      "200/200 - 327s - loss: 0.1116 - accuracy: 0.9716 - 327s/epoch - 2s/step\n",
      "Epoch 25/30\n",
      "200/200 - 137s - loss: 0.0964 - accuracy: 0.9767 - 137s/epoch - 687ms/step\n",
      "Epoch 26/30\n",
      "200/200 - 96s - loss: 0.1067 - accuracy: 0.9731 - 96s/epoch - 482ms/step\n",
      "Epoch 27/30\n",
      "200/200 - 97s - loss: 0.1001 - accuracy: 0.9748 - 97s/epoch - 487ms/step\n",
      "Epoch 28/30\n",
      "200/200 - 98s - loss: 0.0930 - accuracy: 0.9769 - 98s/epoch - 489ms/step\n",
      "Epoch 29/30\n",
      "200/200 - 103s - loss: 0.1098 - accuracy: 0.9737 - 103s/epoch - 514ms/step\n",
      "Epoch 30/30\n",
      "200/200 - 102s - loss: 0.1176 - accuracy: 0.9747 - 102s/epoch - 512ms/step\n"
     ]
    }
   ],
   "source": [
    "#Compiling the model\n",
    "opt = keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.9,\n",
    "    epsilon = 1e-08,\n",
    "    weight_decay=0.0)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Fit the model\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=32),\n",
    "    epochs = 30, \n",
    "    verbose = 2, \n",
    "    steps_per_epoch = 200,\n",
    ")\n",
    "model.save(\"./assets/ocr_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.9973753094673157\n"
     ]
    }
   ],
   "source": [
    "# Image quality is 64x64 pixels in the model\n",
    "model = keras.models.load_model(\"./assets/ocr_model.h5\")\n",
    "score = model.evaluate(test_X, test_y, verbose=0)\n",
    "print('Test Accuracy =', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.9947506785392761\n"
     ]
    }
   ],
   "source": [
    "# Image quality is 32x32 pixels in the small model\n",
    "model = keras.models.load_model(\"./assets/ocr_model_small.h5\")\n",
    "score = model.evaluate(test_X, test_y, verbose=0)\n",
    "print('Test Accuracy =', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
